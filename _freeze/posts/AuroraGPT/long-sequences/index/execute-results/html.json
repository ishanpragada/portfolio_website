{
  "hash": "b359ba8ed776a9db49105597770807db",
  "result": {
    "engine": "jupyter",
    "markdown": "---\n# sidebar: false\ntitle: \"üöÇ Loooooooong Sequence Lengths\"\n# created: \"2023-09-08\"\ndate: \"2024-02-12\"\ncallout-appearance: simple\n# https://www.anl.gov/sites/www/files/2021-09/CPA_RESIZE_Climate%20Resilience%20Images_01_1920x1080.jpg\neditor:\n   render-on-save: true\nexecute:\n   freeze: auto\ncategories:\n  - AuroraGPT\n# lightbox: auto\n# format:\n#   # html:\n#   html:\n#     format: default\n#   gfm:\n#     author: Sam Foreman\n#     output-file: \"deepspeed4science-genslm.md\"\n---\n\n\n\n\n\n::: {#fig-ds4sci style=\"text-align:center;\"}\n\n[![](https://raw.githubusercontent.com/saforem2/llm-lunch-talk/main/docs/assets/ds4sci.svg)]{.stretch}\n\n\nThis work was done as part of the DeepSpeed4Science project, in collaboration with Microsoft.\n:::\n\nThe new [Megatron-DeepSpeed](https://github.com/microsoft/Megatron-DeepSpeed)\nrelease contains a variety of improvements / optimizations to enable\npre-training Transformer based architectures with significantly longer\nsequences than was previously possible.\n\n<!-- > **Note**<br> -->\n<!-- > Additional details can be found in the -->\n<!-- > [üìÅ `DeepSpeed4Science`](https://github.com/microsoft/Megatron-DeepSpeed/examples_deepspeed/deepspeed4science/megatron_long_seq_support/README.md) -->\n<!-- > folder. -->\n\n::: {.callout-note icon=false title=\"üìì Note:\" collapse=\"false\" style=\"width:100%; background: none!important; border: none!important; border-left: 2px solid var(--callout-note-color)!important; border-radius: 0pt!important; opacity: 100%;\"}\n\nAdditional details can be found in the\n[üìÅ `DeepSpeed4Science`](https://github.com/microsoft/Megatron-DeepSpeed/tree/main/examples_deepspeed/deepspeed4science/megatron_long_seq_support)\nfolder.\n\n:::\n\n\n## [DeepSpeed4Science](https://ai4science.azurewebsites.net/2023/09/18/model-showcase-genslms/) (09/2023)\n\n### New Features\n\n- Enabled Megatron-LM's sequence parallel.\n\n- Enabled rotary positional embedding.\n\n- Enabled FlashAttention v1 and v2.\n\n- Enabled new fused kernels from NVIDIA.\n\n### New optimizations\n\n- Enabled attention map memory optimization, where we first generated\nattention mask on CPU memory and then moved it into GPU memory to avoid\nout-of-memory errors when training with very large sequence lengths.\n\n- Position embedding partitioning, where we split weights of position\nencoding across all GPUs when enabling sequence parallel to further reduce\nthe memory footprint.\n\n### Initial Results\n\n| Sequence Length | Old Megatron-DeepSpeed (TFLOPS) | New Megatron-DeepSpeed (TFLOPS) |\n|:---------------:|:--------------------------------:|:--------------------------------:|\n| 2k              | [25]{style=\"text-weight:600;\"}                       | [68]{style=\"text-weight:600;\"}                  |\n| 4k              | [28]{style=\"text-weight:600;\"}          | [80]{style=\"text-weight:600;\"}                   |\n| 8k              | [OOM]{.red-text}                              | [86]{style=\"text-weight:600;\"}                    |\n| 16k             | [OOM]{.red-text}                              | [92]{style=\"text-weight:600;\"}                    |\n| 32k             | [OOM]{.red-text}                              | [100]{style=\"text-weight:600;\"}                 |\n| 64k             | [OOM]{.red-text}                              | [106]{style=\"text-weight:600;\"}                  |\n| 128k            | [OOM]{.red-text}                              | [119]{style=\"text-weight:600;\"}                  |\n| 256k            | [OOM]{.red-text}                              | [94]{style=\"text-weight:600;\"}                    |\n\n: Long sequence length support[^settings] from [`microsoft/Megatron-DeepSpeed`](https://github.com/microsoft/Megatron-DeepSpeed) {#tbl-results .striped .hover}\n\n[^settings]: The described experiments were performed on 4 NVIDIA DGX A100-40GB nodes, all\nusing `TPSIZE=32`[^tpsize], connected through 8 HDR InfiniBand (200Gb/s per\nHDR). \n\n[^tpsize]:|\n  TP stands for `tensor-model-parallel-size` parallelism.\n\n::: {#158d3917 .cell execution_count=1}\n\n::: {.cell-output .cell-output-stdout}\n```\nFailed to download font: IBM Plex Sans, skipping!\nFailed to download font: IBM Plex Sans Condensed, skipping!\nFailed to download font: IBM Plex Serif, skipping!\n```\n:::\n:::\n\n\n::: {#84380e57 .cell execution_count=2}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Data\"}\ngpus = ('32', '64', '128')\n\ncolors = {\n    'Old Megatron-DS': '#FF5252',\n    'Megatron-LM': '#76b900',\n    'New Megatron-DS':  '#1A8FFF',\n}\n\ndata = {\n    '25B': {\n        'Old Megatron-DS': np.array([36, 42, 42]),\n        'Megatron-LM': np.array([26, 48, 52]),\n        'New Megatron-DS': np.array([192, 448, 512]),\n    },\n    '33B': {\n        'Old Megatron-DS': np.array([28, 32, 32]),\n        'Megatron-LM': np.array([14, 46, 52]),\n        'New Megatron-DS': np.array([128, 384, 448]),\n    },\n}\n```\n:::\n\n\n::: {#fig-seq-len style=\"background-color:none;\"}\n\n::: {#c78610f2 .cell layout-nrow='1' layout-ncol='2' execution_count=3}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Make the plots\"}\nx = np.arange(len(gpus))\nwidth = 0.25\nmultiplier = 0\n\noutdir = Path(os.getcwd()).joinpath('assets')\noutdir.mkdir(exist_ok=True, parents=True)\n\nimprovement = {}\nfor idx, (model_size, d) in enumerate(data.items()):\n    multiplier = 0\n    figure, axes = plt.subplots(figsize=(7.5, 4))\n    fig = plt.gcf()\n    ax = plt.gca()\n    for label, value in d.items():\n        offset = width * multiplier\n        rects = ax.barh(\n          x + offset,\n          value,\n          width,\n          label=label,\n          color=colors[label],\n          alpha=0.8\n        )\n        ax.bar_label(\n          rects,\n          padding=3,\n          color=colors[label],\n          family='monospace',\n          weight='bold'\n        )\n        multiplier += 1\n    ax.set_ylabel(\n        'GPUs',\n        fontsize=18,\n        family='sans-serif',\n        loc='center',\n    )\n    ax.set_yticks(x + width, gpus)\n    plt.figtext(\n        0.005, 0.93, f\"{model_size}\", fontsize=24, fontweight='bold', ha='left'\n    )\n    ax.set_xlabel(\n        'Sequence Length (k)', fontsize=18, loc='center'\n    )\n    ax.legend(\n        bbox_to_anchor=(0.005, 1.04, 0.99, .098),\n        alignment='center',\n        edgecolor=\"#83838320\",\n        frameon=True,\n        ncols=3,\n        fontsize=13,\n        mode=\"expand\",\n        borderaxespad=0.01\n    )\n    save_figure(fname=f'{model_size}', outdir=outdir)\n    _ = plt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![GPT-`25B` Model](index_files/figure-html/cell-4-output-1.svg){}\n:::\n\n::: {.cell-output .cell-output-display}\n![GPT-`33B` Model](index_files/figure-html/cell-4-output-2.svg){}\n:::\n:::\n\n\nPre-training with long sequence support across different model sizes and numbers\nof GPUs.\nIn each case, the `new` (current) implementation **significantly**\noutperforms both NVIDIA/Megatron-LM as well as our previous implementation.\n:::\n\n\n## Installation\n\n### Using [`install.sh`](https://github.com/ramanathanlab/genslm/blob/foremans/ds4sci/examples/long-sequences/install.sh)\n\n::: {.callout-tip title=\"Installation\" collapse=\"false\" style=\"width:100%;\"}\n\n**Important**<br>\nTo install, simply:\n```bash\ngit clone https://github.com/ramanthanlab/GenSLM/\ncd GenSLM/examples/long-sequences/\n./install.sh\n``````\nExplicitly,\n[`./install.sh`](https://github.com/ramanathanlab/genslm/blob/foremans/ds4sci/examples/long-sequences/install.sh)\nwill:\n\n1. **Automatically** create a virtual environment _on top of_ the latest `conda` module\n2. Install (+ update[^update]) / build all the required [dependencies](#dependencies) into this virtual environment\n\n:::\n\n[^update]:|\n  2. `deepspeed-0.10.3`\n  1. `pytorch==2.0.0+cu118`\n\n\n### Step-by-Step\n\nFor completeness, we describe below the steps for installing and building each\nof the dependencies.\n\n1. Clone GitHub repo:\n   ```bash\n   git clone https://github.com/ramanthanlab/GenSLM\n   ``````\n\n2. Load `conda` module:\n    - ThetaGPU:\n      ```bash\n      # ThetaGPU:\n      if [[ \"$(hostname)==theta*\" ]]; then\n          export MACHINE=\"ThetaGPU\"\n          export CONDA_DATE=\"2023-01-10\"\n          module load conda/2023-01-11\n          conda activate base\n      fi\n      ```\n    - Polaris:\n      ```bash\n      # Polaris:\n      if [[ \"$(hostname)==x3*\" ]]; then\n          export MACHINE=\"Polaris\"\n          export CONDA_DATE=\"2023-01-10\"\n          module load conda/2023-01-10-unstable\n          conda activate base\n      fi\n      ```\n\n3. Setup Virtual Environment[^venv]:\n   ```bash\n   cd ./genslm/examples/long-sequences\n   # create a new virtual environment\n   mkdir -p \"venvs/${MACHINE}/${CONDA_DATE}\"\n   python3 -m venv \"venvs/${MACHINE}/${CONDA_DATE}\" --system-site-packages\n   source \"venvs/${MACHINE}/${CONDA_DATE}/bin/activate\"\n   ``````\n\n4. Create a new folder (`genslm/examples/long-sequences/deps/${MACHINE}`) where\n   we'll installing dependencies locally:\n   ```bash\n   mkdir -p \"deps/${MACHINE}\"\n   cd \"deps/${MACHINE}\"\n   ```\n\n[^venv]: Where `\"${MACHINE}\"` $\\in$ `{\"ThetaGPU\", \"Polaris\"}` and\n  `\"${CONDA_DATE}\"` $\\in$ `{\"2023-01-10\", \"2023-01-11\"}`\n\n#### Dependencies\n\nWe provide below the details needed to install each of the required dependencies.\n\n<details>\n<summary>[{{< fa brands github >}} `saforem2/ezpz`](https://github.com/saforem2/ezpz)</summary>\n\n1. [{{< fa brands github >}} `saforem2/ezpz`](https://github.com/saforem2/ezpz)\n   ```bash\n   pip install -e \"git+https://github.com/saforem2/ezpz.git#egg=ezpz\"\n   ```\n\n</details>\n\n<details>\n<summary>[{{< fa brands github >}} `Microsoft/DeepSpeed`](https://github.com/microsoft/DeepSpeed)\n</summary>\n\n2. [{{< fa brands github >}} `Microsoft/DeepSpeed`](https://github.com/microsoft/DeepSpeed)\n   ```bash\n   git clone https://github.com/microsoft/DeepSpeed.git\n   cd DeepSpeed\n   python3 -m pip install -e .\n   ``````\n</details>\n\n<details>\n<summary>[{{< fa brands github >}} `Microsoft/Megatron-DeepSpeed`](https://github.com/microsoft/Megatron-DeepSpeed)\n</summary>\n\n3. [{{< fa brands github >}} `Microsoft/Megatron-DeepSpeed`](https://github.com/microsoft/Megatron-DeepSpeed):\n   ```bash\n   git clone https://github.com/microsoft/Megatron-DeepSpeed.git\n   ```\n</details>\n\n\n<details>\n<summary> [{{< fa brands github >}} `NVIDIA/apex`](https://github.com/NVIDIA/apex)\n</summary>\n\n4. [{{< fa brands github >}} `NVIDIA/apex`](https://github.com/NVIDIA/apex)\n   ```bash\n   git clone https://github.com/NVIDIA/apex\n   cd ../apex/\n   pip install -v --disable-pip-version-check --no-cache-dir --no-build-isolation --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" -e ./\n   ``````\n</details>\n\n<details>\n<summary>[{{< fa brands github >}} `pybind/PyBind11`](https://github.com/pybind/pybind11)</summary>\n\n5. [{{< fa brands github >}} `pybind/PyBind11`](https://github.com/pybind/pybind11)\n   ```bash\n   pip install pybind11\n   ``````\n\n</details>\n\n<details>\n<summary>[{{< fa brands github >}} `Dao-AILab/flash-attention`](https://github.com/Dao-AILab/flash-attention)</summary>\n\n6. [{{< fa brands github >}} `Dao-AILab/flash-attention`](https://github.com/Dao-AILab/flash-attention):\n\n   ::: {.callout-caution title=\"Flash Attention\" collapse=\"true\" style=\"font-size:0.8em; width:100%;\"}\n    - The new release supports three different implementations of\n      FlashAttention: (`v1.0.4`, `v2.x`, `triton`)\n    - FlashAttention `v2.x` may have numerical instability issues.\n      For the best performance, we recommend using FlashAttention + Triton\n   :::\n\n   - `v1.0.4`:\n     ```bash\n     python3 -m pip install flash-attn==1.0.4\n     ```\n   - `v2.x`:\n     ```bash\n     git clone https://github.com/Dao-AILab/flash-attention\n     cd flash-attention\n     python3 setup.py install\n     ```\n   - `openai/triton`:\n     ```bash\n     git clone -b legacy-backend https://github.com/openai/triton\n     cd triton/python\n     python3 -m pip install cmake\n     python3 -m pip install .\n     ```\n\n</details>\n\n## Running\n\nThe [`ALCF/`](./ALCF/) directory contains shell scripts for setting up the\nenvironment and specifying the options to be used when launching.\n\nVarious options can be specified dynamically at runtime by setting them in your\nenvironment, e.g.:\n\n```bash\nMODEL_SIZE_KEY=\"GPT25B\" SEQ_LEN=128000 USE_FLASH_ATTN=1 MICRO_BATCH=1 GAS=1 SP_TYPE=\"megatron\" ZERO_STAGE=1 ./ALCF/train-gpt3.sh\n``````\n\nExplicitly:\n\n- [`ALCF/train-gpt3.sh`](./ALCF/train-gpt3.sh): **Main entry point for training**\n  - This script will **automatically** source the rest of the required [`ALCF/*.sh`](./ALCF/) scripts below\n- [`ALCF/models.sh`](./ALCF/models.sh): Contains some example model architectures for GPT3-style models\n- [`ALCF/args.sh`](./ALCF/args.sh): Logic for parsing / setting up runtime options for Megatron and DeepSpeed\n- [`ALCF/setup.sh`](./ALCF/args.sh): Locate and activate virtual environment to be used, ensure MPI variables are set properly\n- [`ALCF/launch.sh`](./ALCF/launch.sh): Identify available resources and build the command to be executed\n  - i.e. figure out how many: `{nodes, GPUs per node, GPUs total}`, to pass to `mpi{run,exec}`\n  - then, use this to build `mpiexec <mpiexec-args> python3 pretrain_gpt.py`\n\n\n## ZeRO Offloading\n\n[üöÄ **W&B Report**: _Looooooooong Sequences_](https://wandb.ai/l2hmc-qcd/Megatron-DS-Benchmarking/reports/Looooooong-Sequences--Vmlldzo1MzI2NjA1)\n\nThese newly introduced optimizations, in combination with\n[ZeRO-Offload](https://www.deepspeed.ai/tutorials/zero-offload/) allows us to go even further.\n\nBy employing ZeRO-Offloading, we are able to free up additional memory which\ncan be used for _even longer_ sequences.\n\nThough work is still ongoing, this is a promising direction that will allow us\nto consider significantly larger genomes than previously possible.\n\nWe use [Weights \\& Biases](https://wandb.ai) to track these experiments, and\nhave aggregated our initial results in the [W\\&B\nReport](https://wandb.ai/l2hmc-qcd/Megatron-DS-Benchmarking/reports/Looooooong-Sequences--Vmlldzo1MzI2NjA1)\nbelow.\n\nWe can evaluate the performance of our model by looking at two different\nmetrics for throughput: `samples_per_sec` and `TFLOPS`.\n\nExplicitly, we see that we are able to scale up to significantly longer\nsequences (`420k / 128k ~ 3.3x`) with only a minimal impact on throughput\nperformance (`81 / 105 ~ 77\\%`)[^tflops-scaling].\n\n\n|  Name  | Sequence Length (k) | (`seq_len / min_seq_len`) |  TFLOPS  | TFLOPS (% of peak) |\n|:------:|:-------------------:|:-----------------------:|:--------:|:------------------:|\n| GPT25B |         420         |         [**3.28125**]{.blue-text}         | 81.77225 |       [**77.867**]{.blue-text}       |\n| GPT25B |         400         |          3.125          |  90.62   |       86.297       |\n| GPT25B |         360         |         2.8125          | 81.6325  |      77.7348       |\n| GPT25B |         360         |         2.8125          | 82.6824  |      78.7346       |\n| GPT25B |         192         |           1.5           | 115.8228 |      110.2927      |\n| GPT25B |         128         |            1            | 106.672  |      101.5788      |\n| GPT25B |         128         |            1            | 105.014  |       100.00       |\n\n: Impact on TFLOPS as a function of increasing sequence length. Table from: [`throughput/TFLOPS`](https://api.wandb.ai/links/l2hmc-qcd/awklywn7) {#tbl-seqlen .striped .hover}\n\n<!-- [^config]: Using: `{model_size: 25B, WORLD_SIZE: 32, micro_batch: 1}` -->\n[^tflops-scaling]: [`throughput/TFLOPS`](https://api.wandb.ai/links/l2hmc-qcd/awklywn7)\n\n<!-- <iframe src=\"https://wandb.ai/l2hmc-qcd/Megatron-DS-Benchmarking?workspace=user-saforem2\" style=\"border:none;height:1024px;width:100%\"> -->\n\n::: {#fig-wandb}\n\n:::  {style=\"padding:0.5rem; border: 1px solid var(--dim-text); border-radius: 0.2rem;\"}\n\n<iframe src=\"https://wandb.ai/l2hmc-qcd/Megatron-DS-Benchmarking/reports/Looooooong-Sequences--Vmlldzo1MzI2NjA1\" style=\"border:none;height:1024px;width:100%\">\n</iframe>\n\n:::\n\nWeights \\& Biases Report\n\n:::\n\n",
    "supporting": [
      "index_files/figure-html"
    ],
    "filters": [],
    "includes": {}
  }
}